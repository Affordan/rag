
# 0. å‡†å¤‡æ–‡æ¡£

# 1. Loading æ–‡æ¡£åŠ è½½å™¨ï¼ŒåŠ è½½åˆ°Langchainä¸­
# pip install pypdf
# pip install docx2txt
import os
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader,TextLoader
import torch

# æ–‡æ¡£åŠ è½½
base_dir ="documents"

doucuments = []

for file_name in os.listdir(base_dir):
    file_path = os.path.join(base_dir, file_name)
    if file_name.endswith(".txt"):
        loader = TextLoader(file_path, encoding='utf-8')
        doucuments.extend(loader.load())
    elif file_name.endswith(".pdf"):
        loader = PyPDFLoader(file_path)
        doucuments.extend(loader.load())
    elif file_name.endswith(".docx"):
        loader = Docx2txtLoader(file_path)
        doucuments.extend(loader.load())

# 2. Spliting: åˆ‡åˆ†ã€å°†åŠ è½½çš„æ–‡æ¡£è¿›è¡Œåˆ†ç‰‡é¢„å¤„ç†
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=10)

chunked_documents = text_splitter.split_documents(doucuments)
# print(chunked_documents)

# 3. Storrage: VectorDB --embedding
# 3-1. embeddingmodel
# 0. package install
# pip install sentence-transformers
# pip install transformers torch
# pip install langchain-huggingface
# 1. download embeddingmodel
# git lfs install
# git -c htts.sslVerify=false clone https://hf-mirror.com/moka-ai/moka-embedding-zh-base-v1

# 2. å°è£…
from langchain_huggingface import HuggingFaceEmbeddings

m3e_name = "./embedding_models/moka/m3e-base"
bce_name ="./embedding_models/netease-youdao"

# æŒ‡å®šæ¨¡å‹è¿è¡Œçš„è®¾å¤‡
model_kwargs = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}

# ç¼–ç æ—¶ï¼Œå®Œæˆå½’ä¸€åŒ–
encode_kwargs = {
    'normalize_embeddings': True,  # æ˜¯å¦å½’ä¸€åŒ–åµŒå…¥å‘é‡
}

# ä½¿ç”¨HuggingFaceEmbeddingsåŠ è½½æ¨¡å‹
print("ğŸ¤– æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹ï¼Œè¯·ç¨å€™...")
embedding_model = HuggingFaceEmbeddings(
    model_name=m3e_name, # use m3e-base for better performance,can also use bce-base
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)
print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")

# 3-2. storage -> vectordb
# å‘é‡æ•°æ®åº“å¾ˆå¤šï¼Œä½†æ˜¯ä½ å¯ä»¥è‡ªå·±å°è¯•ä½¿ç”¨FAISS, ChromaDB, Weaviateç­‰
# è¿™é‡Œä½¿ç”¨ qdrant ä¸ºä¾‹
# pip install qdrant-client

from langchain_community.vectorstores import FAISS
import pickle
import os

# å‘é‡æ•°æ®åº“æ–‡ä»¶è·¯å¾„
vector_db_path = "./vector_db"
vectorstore_file = os.path.join(vector_db_path, "faiss_index")

# æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å‘é‡æ•°æ®åº“
if os.path.exists(vectorstore_file + ".pkl"):
    print("ğŸ“‚ å‘ç°å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“ï¼Œæ­£åœ¨åŠ è½½...")
    vectorstore = FAISS.load_local(vector_db_path, embedding_model, allow_dangerous_deserialization=True)
    print("âœ… å‘é‡æ•°æ®åº“åŠ è½½å®Œæˆ")
else:
    print("ğŸ’¾ æ­£åœ¨æ„å»ºFAISSå‘é‡æ•°æ®åº“...")
    vectorstore = FAISS.from_documents(
        documents=chunked_documents,
        embedding=embedding_model
    )
    
    # ä¿å­˜å‘é‡æ•°æ®åº“åˆ°æœ¬åœ°
    os.makedirs(vector_db_path, exist_ok=True)
    vectorstore.save_local(vector_db_path)
    print("âœ… å‘é‡æ•°æ®åº“æ„å»ºå¹¶ä¿å­˜å®Œæˆ")

# 4. Retrieval: æ£€ç´¢ chat_model,qachain
# chat_model ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨ä½™å¼¦è·ç¦»æ¥åº¦é‡æ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦

# 4-1. chat_model
# pip install langchain[all] langchain-openai
# pip install pydev
'''
    apply a gemini api key here
    we use deepseek api now to see if it works
'''

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.retrievers.multi_query import MultiQueryRetriever
from pydantic import SecretStr
load_dotenv()

print("ğŸ”— æ­£åœ¨è¿æ¥åƒé—®æ¨¡å‹...")
chat_model = ChatOpenAI(
    model='qwen-turbo',
    base_url=os.environ["BAILIAN_API_URL"],
    api_key=SecretStr(os.environ["BAILIAN_API_KEY"])
)
print("âœ… åƒé—®æ¨¡å‹è¿æ¥å®Œæˆ")

# 4-2. RetrievalQA chain

retriever_from_llm = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    llm=chat_model,
)
from langchain.chains import RetrievalQA

# å®ä¾‹åŒ–ä¸€ä¸ª RetrivalQA é“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=chat_model,
    chain_type="stuff",  # ä½¿ç”¨ "stuff" é“¾ç±»å‹
    retriever=retriever_from_llm,
    return_source_documents=True,  # è¿”å›æºæ–‡æ¡£
    verbose=True  # å¯ç”¨è¯¦ç»†æ—¥å¿—
)


# 5. UI: I-<qachain>-0

# question = "è¯·é—®'è½æ—¥å…­å·'åœç•™åœ¨åœ°çƒçš„å“ªé‡Œï¼Ÿ"
# response = qa_chain({"query": question})
# print("å›ç­”:", response['result'])
# print("ç›¸å…³æ–‡æ¡£:")
# print(response['source_documents'])
# for doc in response['source_documents']:
#     print(doc)


# åç«¯

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
import requests
import uvicorn
from typing import List, Dict, Any

app = FastAPI(title="æ–‡æ¡£é—®ç­”ç³»ç»Ÿ", description="åŸºäºAIçš„æ–‡æ¡£æ£€ç´¢ä¸é—®ç­”API")

# è®¾ç½®æ¨¡æ¿ç›®å½•
templates = Jinja2Templates(directory="templates")

# å®šä¹‰è¯·æ±‚æ¨¡å‹
class QuestionRequest(BaseModel):
    question: str

class AnswerResponse(BaseModel):
    answer: str
    source_documents: List[Dict[str, Any]]
    question: str

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    """ä¸»é¡µé¢"""
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/ask", response_model=AnswerResponse)
async def ask_question(question_request: QuestionRequest):
    """å¤„ç†é—®ç­”è¯·æ±‚"""
    try:
        question = question_request.question.strip()
        if not question:
            raise HTTPException(status_code=400, detail="é—®é¢˜ä¸èƒ½ä¸ºç©º")
        
        # ä½¿ç”¨ qa_chain è¿›è¡Œé—®ç­”
        response = qa_chain({"query": question})
        
        # æ ¼å¼åŒ–æºæ–‡æ¡£
        source_docs = []
        if response.get('source_documents'):
            for doc in response['source_documents']:
                source_docs.append({
                    "page_content": doc.page_content,
                    "metadata": doc.metadata
                })
        
        return AnswerResponse(
            answer=response['result'],
            source_documents=source_docs,
            question=question
        )
        
    except Exception as e:
        print(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {str(e)}")

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy", "message": "æ–‡æ¡£é—®ç­”ç³»ç»Ÿè¿è¡Œæ­£å¸¸"}

@app.get("/docs_count")
async def get_documents_count():
    """è·å–æ–‡æ¡£æ•°é‡ä¿¡æ¯"""
    try:
        # è·å–å‘é‡æ•°æ®åº“ä¸­çš„æ–‡æ¡£æ•°é‡
        doc_count = len(chunked_documents)
        return {
            "total_documents": len(doucuments),
            "chunked_documents": doc_count,
            "status": "ready"
        }
    except Exception as e:
        return {"error": str(e)}

if __name__ == "__main__":
    print("ğŸš€ æ­£åœ¨å¯åŠ¨æ–‡æ¡£é—®ç­”ç³»ç»Ÿ...")
    print("â³ ç³»ç»Ÿæ­£åœ¨åˆå§‹åŒ–ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    print("ğŸ“š æ–‡æ¡£åŠ è½½å®Œæˆ")
    print("ğŸ¤– AIæ¨¡å‹å‡†å¤‡å°±ç»ª")
    print("ğŸŒ Webç•Œé¢: http://localhost:8000")
    print("ğŸ“– APIæ–‡æ¡£: http://localhost:8000/docs")
    print("âœ… ç³»ç»Ÿå¯åŠ¨å®Œæˆï¼")
    
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000,
        reload=False  # ç”Ÿäº§ç¯å¢ƒå»ºè®®è®¾ä¸ºFalse
    )